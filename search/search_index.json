{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ImGRAFT An image georectification and feature tracking toolbox for MATLAB This software is open source (See licensing details elsewhere). In addition to the formal licensing terms, We would greatly appreciate an acknowledgement. Preferably in the form of a citation and a link to the web-page. Citation: Messerli, A. and Grinsted, A. (2015), Image GeoRectification And Feature Tracking toolbox: ImGRAFT, Geosci. Instrum. Method. Data Syst., 4, 23-34, doi: 10.5194/gi-4-23-2015 Features Feature tracking between image pairs using template matching. Full processing line from feature tracking to georectification. Distorted camera model. This allows the use of cheaper camera setups. Avoiding traditional image registration as a pre-processing step as it degrades the images due to resampling. ImGRAFT will instead optimize camera view for each image. Projecting between pixel and real world coordinates. 2D \u2194 3D Easily scriptable as it is a toolbox. Minimal dependencies. (No other toolboxes required). Inputs to feature tracking: two images Inputs to georectification: a DEM and some ground control points. We hope you will find this package useful. We would be grateful for any feedback and example use-cases. Authors: Aslak Grinsted & Alexandra Messerli Licensing The majority of the code is licensed under a very permissive MIT license, but some routines and example data are licensed under other terms. See licensing details in LICENSE.txt and individual files. This software package includes the following open source codes licensed under other terms: LMFnlsq.m Copyright Miroslav Balda. This is an implementation of the Levenberg-Marquardt algorithm as modified by Fletcher. It is used in the least squares optimization of the camera parameters. See licensing details in LMFnlsq. Acknowledgements This software has been developed at Centre for Ice and Climate , Niels Bohr Institute, University of Copenhagen as part of the SVALI project . SVALI is a part of the Top-level Research Initiative (TRI), which is a major Nordic collaborative venture for studies of climate, energy and the environment. We are also grateful to Miriam Jackson and NVE who has helped facilitate the Engabreen fieldwork and contributed with data.","title":"Frontpage"},{"location":"#imgraft","text":"An image georectification and feature tracking toolbox for MATLAB This software is open source (See licensing details elsewhere). In addition to the formal licensing terms, We would greatly appreciate an acknowledgement. Preferably in the form of a citation and a link to the web-page. Citation: Messerli, A. and Grinsted, A. (2015), Image GeoRectification And Feature Tracking toolbox: ImGRAFT, Geosci. Instrum. Method. Data Syst., 4, 23-34, doi: 10.5194/gi-4-23-2015","title":"ImGRAFT"},{"location":"#features","text":"Feature tracking between image pairs using template matching. Full processing line from feature tracking to georectification. Distorted camera model. This allows the use of cheaper camera setups. Avoiding traditional image registration as a pre-processing step as it degrades the images due to resampling. ImGRAFT will instead optimize camera view for each image. Projecting between pixel and real world coordinates. 2D \u2194 3D Easily scriptable as it is a toolbox. Minimal dependencies. (No other toolboxes required). Inputs to feature tracking: two images Inputs to georectification: a DEM and some ground control points. We hope you will find this package useful. We would be grateful for any feedback and example use-cases. Authors: Aslak Grinsted & Alexandra Messerli","title":"Features"},{"location":"#licensing","text":"The majority of the code is licensed under a very permissive MIT license, but some routines and example data are licensed under other terms. See licensing details in LICENSE.txt and individual files. This software package includes the following open source codes licensed under other terms: LMFnlsq.m Copyright Miroslav Balda. This is an implementation of the Levenberg-Marquardt algorithm as modified by Fletcher. It is used in the least squares optimization of the camera parameters. See licensing details in LMFnlsq.","title":"Licensing"},{"location":"#acknowledgements","text":"This software has been developed at Centre for Ice and Climate , Niels Bohr Institute, University of Copenhagen as part of the SVALI project . SVALI is a part of the Top-level Research Initiative (TRI), which is a major Nordic collaborative venture for studies of climate, energy and the environment. We are also grateful to Miriam Jackson and NVE who has helped facilitate the Engabreen fieldwork and contributed with data.","title":"Acknowledgements"},{"location":"alphawarp/","text":"alphawarp Drapes a semi-transparent and possibly distorted image over a figure. This can be used to display a variable on top of a background image. USAGE: h=alphawarp(x,y,c,alpha) EXAMPLE: P=imread('peppers.png'); P=repmat(mean(im2double(P),3),[1 1 3]);%make a (gray) RGB-image so that it does not influence colorbar image(P); [X,Y]=meshgrid(100:300,100:300); X=X+Y*.1; Z=peaks((X-150)/100,(Y-150)/100); alphawarp(X,Y,Z,.5);","title":"Alphawarp"},{"location":"alphawarp/#alphawarp","text":"Drapes a semi-transparent and possibly distorted image over a figure. This can be used to display a variable on top of a background image. USAGE: h=alphawarp(x,y,c,alpha) EXAMPLE: P=imread('peppers.png'); P=repmat(mean(im2double(P),3),[1 1 3]);%make a (gray) RGB-image so that it does not influence colorbar image(P); [X,Y]=meshgrid(100:300,100:300); X=X+Y*.1; Z=peaks((X-150)/100,(Y-150)/100); alphawarp(X,Y,Z,.5);","title":"alphawarp"},{"location":"camera/","text":"Camera.m The camera model holds both all the internal camera parameters (focal lengths, distortion coefficients, etc) and all the external camera parameters (camera location and view direction). This class can be used to project back and forth between pixel and world coordinates once the camera parameters have been specified. It can also be used to calibrate the camera parameters given a set of ground control points. A good example of how the camera class is used can be found in the Engabreen example. imgsz - size of image in pixels [#rows, #columns] camera properties: f - focal length in pixel units (two element vector [fx,fy]) c - camera center in pixel coordinates (two element vector: [cx,cy]) k - radial distortion coefficients. (six element vector: [k1-k6]) p - tangential distortion coefficients (two element vector: [p1,p2]) xyz - world coordinates of camera. viewdir - [yaw,pitch,roll]. Yaw: rotation about z (0=looking east) Pitch: look up/down angle Roll: camera roll (horizon tilt). camera Dependent/derived properties: (i.e. properties calculated from the camera parameters) R - camera rotation matrix calculated from camera view direction (read only) fullmodel - a 20-element vector containing all camera properties. [camx,camy,camz,imgszy,imgszx,viewdiryaw,viewdirpitch,viewdirroll,fx,fy,cx,cy,k1-6,p1-2] camera Methods: camera - constructor optimizecam - optimize the camera to mimimize misfit between projected world coordinates and pixel coordinates. project - project world coordinates to image coordinates (3d->2d) invproject - project image coordinates to world coordinates (2d->3d) Setting up camera parameters You setup the camera parameters using its contructor and then you can modify the parameters afterwards. E.g. like this: cam=camera([446722 7396671 770]) %specifying world coordinates only cam.f=[6000 6000]; cam.viewdir=[90 0 0]*pi/180; %looking north The Engabreen and Schneefernerkopf examples show some practical use cases. There are additional ways the constructor can be called. Type \"help camera.camera\" on the matlab prompt to see the full help of the constructor. Calibrating camera parameters from ground control points The camera class contains an optimizeCam method which can be used to calibrate the camera and also determine the external camera parameters. You give it a set of ground control points. I.e. a set of 3d world coordinates and corresponding pixel coordinates. OptimizeCam will then work to minimize the misfit between the projected 3d points and the pixel coordinates. You can specify which parameters which are free in the optimization. [newcamera,rmse,AIC] = cam.optimizecam(worldcoordinates,pixelcoordinates,'00000111000000000000') In this case the string '00000111000000000000' says that the 6,7,8th parameters should be free. The ordering here refers to the ordering in the \"fullmodel\" where 6-8 corresponds to the three view direction angle. So, in this case we are only optimizing the view direction of the camera. The Engabreen and Schneefernerkopf examples show how a simple radial distortion model is determined from the ground control points. Type \"help camera.optimizeCam\" on the matlab prompt for more details on how to use it. Projecting from world coordinates to image coordinates (3d to 2d) The project method makes it simple to project from world coordinates to pixel coordinates. [uv,~,inframe]=cam.project(xyz) Here uv are the projected pixel coordinates. inframe is a boolean array which is true if the projected point is within the frame of the image. See the Schneefernerkopf example for a simple example how it can be used. Projecting from image coordinates to world coordinates (2d to 3d) Projecting from 2d to 3d can also be considered raytracing conceptually. Usually you want to find the point on a DEM, that corresponds to the pixel coordinates after projection. You can do this with invproject . xyz=cam.invproject(uv,demX,demY,demZ) The Engabreen example shows how this can be used in practice. Type \"help camera.invproject\" on the matlab prompt for more details on how to use it. How do you calculate the focal length in pixel units from the camera specifications? The focal length from the manufacturer alone does not determine the field of view / zoom level of a lens. The focal length in mm together with sensor size and image size determines the field of view that the lens is able to see. The camera needs the focal length in pixel units. These can be calculated thus: FocalLength=30; %mm (as per lens specifications) SensorSize=[22.0 14.8]; %mm (this is the physical size of the sensor in the camera. It will depend on model.) imgsz=size(A); f=imgsz([2 1]).*(FocalLength./SensorSize);","title":"Camera"},{"location":"camera/#cameram","text":"The camera model holds both all the internal camera parameters (focal lengths, distortion coefficients, etc) and all the external camera parameters (camera location and view direction). This class can be used to project back and forth between pixel and world coordinates once the camera parameters have been specified. It can also be used to calibrate the camera parameters given a set of ground control points. A good example of how the camera class is used can be found in the Engabreen example. imgsz - size of image in pixels [#rows, #columns] camera properties: f - focal length in pixel units (two element vector [fx,fy]) c - camera center in pixel coordinates (two element vector: [cx,cy]) k - radial distortion coefficients. (six element vector: [k1-k6]) p - tangential distortion coefficients (two element vector: [p1,p2]) xyz - world coordinates of camera. viewdir - [yaw,pitch,roll]. Yaw: rotation about z (0=looking east) Pitch: look up/down angle Roll: camera roll (horizon tilt). camera Dependent/derived properties: (i.e. properties calculated from the camera parameters) R - camera rotation matrix calculated from camera view direction (read only) fullmodel - a 20-element vector containing all camera properties. [camx,camy,camz,imgszy,imgszx,viewdiryaw,viewdirpitch,viewdirroll,fx,fy,cx,cy,k1-6,p1-2] camera Methods: camera - constructor optimizecam - optimize the camera to mimimize misfit between projected world coordinates and pixel coordinates. project - project world coordinates to image coordinates (3d->2d) invproject - project image coordinates to world coordinates (2d->3d)","title":"Camera.m"},{"location":"camera/#setting-up-camera-parameters","text":"You setup the camera parameters using its contructor and then you can modify the parameters afterwards. E.g. like this: cam=camera([446722 7396671 770]) %specifying world coordinates only cam.f=[6000 6000]; cam.viewdir=[90 0 0]*pi/180; %looking north The Engabreen and Schneefernerkopf examples show some practical use cases. There are additional ways the constructor can be called. Type \"help camera.camera\" on the matlab prompt to see the full help of the constructor.","title":"Setting up camera parameters"},{"location":"camera/#calibrating-camera-parameters-from-ground-control-points","text":"The camera class contains an optimizeCam method which can be used to calibrate the camera and also determine the external camera parameters. You give it a set of ground control points. I.e. a set of 3d world coordinates and corresponding pixel coordinates. OptimizeCam will then work to minimize the misfit between the projected 3d points and the pixel coordinates. You can specify which parameters which are free in the optimization. [newcamera,rmse,AIC] = cam.optimizecam(worldcoordinates,pixelcoordinates,'00000111000000000000') In this case the string '00000111000000000000' says that the 6,7,8th parameters should be free. The ordering here refers to the ordering in the \"fullmodel\" where 6-8 corresponds to the three view direction angle. So, in this case we are only optimizing the view direction of the camera. The Engabreen and Schneefernerkopf examples show how a simple radial distortion model is determined from the ground control points. Type \"help camera.optimizeCam\" on the matlab prompt for more details on how to use it. Projecting from world coordinates to image coordinates (3d to 2d) The project method makes it simple to project from world coordinates to pixel coordinates. [uv,~,inframe]=cam.project(xyz) Here uv are the projected pixel coordinates. inframe is a boolean array which is true if the projected point is within the frame of the image. See the Schneefernerkopf example for a simple example how it can be used. Projecting from image coordinates to world coordinates (2d to 3d) Projecting from 2d to 3d can also be considered raytracing conceptually. Usually you want to find the point on a DEM, that corresponds to the pixel coordinates after projection. You can do this with invproject . xyz=cam.invproject(uv,demX,demY,demZ) The Engabreen example shows how this can be used in practice. Type \"help camera.invproject\" on the matlab prompt for more details on how to use it. How do you calculate the focal length in pixel units from the camera specifications? The focal length from the manufacturer alone does not determine the field of view / zoom level of a lens. The focal length in mm together with sensor size and image size determines the field of view that the lens is able to see. The camera needs the focal length in pixel units. These can be calculated thus: FocalLength=30; %mm (as per lens specifications) SensorSize=[22.0 14.8]; %mm (this is the physical size of the sensor in the camera. It will depend on model.) imgsz=size(A); f=imgsz([2 1]).*(FocalLength./SensorSize);","title":"Calibrating camera parameters from ground control points"},{"location":"demobatura/","text":"Batura Glacier, Karakoram CIAS is another free feature tracking software. This is a GUI written in IDL by K\u00e4\u00e4b & Vollmer. Here we use ImGRAFT to track one of the CIAS example data sets using orthorectified Landsat 7 images from Batura. Here's how you might track it in ImGRAFT. Note: this example needs the mapping toolbox in order to read the geo-tiffs. datafolder=downloadDemoData('cias'); %%load data % % [A,x,y,Ia]=geoimread(fullfile(datafolder,'batura_2001.tif')); [B,xb,yb,Ib]=geoimread(fullfile(datafolder,'batura_2002.tif')); deltax=x(2)-x(1);%m/pixel deltay=y(2)-y(1);%m/pixel %make regular grid of points to track: [pu,pv]=meshgrid(10:20:size(A,2),10:20:size(A,1)); %pixel coordinated %obtain corresponding map coordinates of pixel coordinates px=interp1(x,pu); py=interp1(y,pv); %... but restricted to points inside this region of interest polygon roi=[387 452;831 543;1126 899;1343 1006;1657 1022;2188 1330;... 2437 1220;2564 1359;2483 1473;2188 1489;1693 1320;1563 1181; ... 1061 1168;663 718;456 686;25 877;28 627;407 465]; mask=inpolygon(pu,pv,roi(:,1),roi(:,2)); pu(~mask)=nan; %inserting nans at some locations will tell template match to skip these locations [du,dv,C,Cnoise,pu,pv]=templatematch(A,B,pu,pv,'showprogress',true,'method','oc'); close all %visualize the results %turn the intensity image into an RGB image %so that it does not interfere with colorbar: showimg(x,y,A) hold on signal2noise=C./Cnoise; keep=(signal2noise>2)&(C>.6); V=(du*deltax)+(dv*1i)*deltay; %m/yr Vn=abs(V); alphawarp(px,py,Vn,.2+keep*.5) quiver(px(keep),py(keep),real(V(keep))./Vn(keep),imag(V(keep))./Vn(keep),0.2,'k') %arrows show direction. caxis([0 200]) colorbar('southoutside');","title":"Batura Glacier, Karakoram"},{"location":"demobatura/#batura-glacier-karakoram","text":"CIAS is another free feature tracking software. This is a GUI written in IDL by K\u00e4\u00e4b & Vollmer. Here we use ImGRAFT to track one of the CIAS example data sets using orthorectified Landsat 7 images from Batura. Here's how you might track it in ImGRAFT. Note: this example needs the mapping toolbox in order to read the geo-tiffs. datafolder=downloadDemoData('cias'); %%load data % % [A,x,y,Ia]=geoimread(fullfile(datafolder,'batura_2001.tif')); [B,xb,yb,Ib]=geoimread(fullfile(datafolder,'batura_2002.tif')); deltax=x(2)-x(1);%m/pixel deltay=y(2)-y(1);%m/pixel %make regular grid of points to track: [pu,pv]=meshgrid(10:20:size(A,2),10:20:size(A,1)); %pixel coordinated %obtain corresponding map coordinates of pixel coordinates px=interp1(x,pu); py=interp1(y,pv); %... but restricted to points inside this region of interest polygon roi=[387 452;831 543;1126 899;1343 1006;1657 1022;2188 1330;... 2437 1220;2564 1359;2483 1473;2188 1489;1693 1320;1563 1181; ... 1061 1168;663 718;456 686;25 877;28 627;407 465]; mask=inpolygon(pu,pv,roi(:,1),roi(:,2)); pu(~mask)=nan; %inserting nans at some locations will tell template match to skip these locations [du,dv,C,Cnoise,pu,pv]=templatematch(A,B,pu,pv,'showprogress',true,'method','oc'); close all %visualize the results %turn the intensity image into an RGB image %so that it does not interfere with colorbar: showimg(x,y,A) hold on signal2noise=C./Cnoise; keep=(signal2noise>2)&(C>.6); V=(du*deltax)+(dv*1i)*deltay; %m/yr Vn=abs(V); alphawarp(px,py,Vn,.2+keep*.5) quiver(px(keep),py(keep),real(V(keep))./Vn(keep),imag(V(keep))./Vn(keep),0.2,'k') %arrows show direction. caxis([0 200]) colorbar('southoutside');","title":"Batura Glacier, Karakoram"},{"location":"demobindschadler/","text":"Bindschadler Ice Stream example The IMCORR feature tracking software comes with two example image pairs. These are Landsat TM subscenes of a portion of Ice Stream D in West Antarctica (Now know as Bindschadler Ice Stream). This page shows how these can be tracked using the ImGRAFT toolbox. datafolder=downloadDemoData('imcorr'); A=imread(fullfile(datafolder,'conv_87.png')); B=imread(fullfile(datafolder,'conv_89.png')); dx=28.5; %m/pixel dt=2; %years x=(0:size(A,2)-1)*dx; y=(size(A,2)-1:-1:0)*dx; tic [du,dv,C,Cnoise,pu,pv]=templatematch(A,B); toc %obtain corresponding map coordinates of pixel coordinates in pu,pv px=interp1(x,pu); py=interp1(y,pv); %display figure close all showimg(x,y,A) hold on signal2noise=C./Cnoise; keep=(signal2noise>3.5); V=(du-dv*1i)*dx/dt; %Using imaginary values to indicate direction (for convenience). Vn=abs(V); alphawarp(px,py,Vn,.2+keep*.5) quiver(px(keep),py(keep),real(V(keep))./Vn(keep),imag(V(keep))./Vn(keep),0.2,'k') %arrows show direction. caxis([0 400]) colorbar('eastoutside'); Elapsed time is 1.346849 seconds.","title":"Bindschadler Ice Stream example"},{"location":"demobindschadler/#bindschadler-ice-stream-example","text":"The IMCORR feature tracking software comes with two example image pairs. These are Landsat TM subscenes of a portion of Ice Stream D in West Antarctica (Now know as Bindschadler Ice Stream). This page shows how these can be tracked using the ImGRAFT toolbox. datafolder=downloadDemoData('imcorr'); A=imread(fullfile(datafolder,'conv_87.png')); B=imread(fullfile(datafolder,'conv_89.png')); dx=28.5; %m/pixel dt=2; %years x=(0:size(A,2)-1)*dx; y=(size(A,2)-1:-1:0)*dx; tic [du,dv,C,Cnoise,pu,pv]=templatematch(A,B); toc %obtain corresponding map coordinates of pixel coordinates in pu,pv px=interp1(x,pu); py=interp1(y,pv); %display figure close all showimg(x,y,A) hold on signal2noise=C./Cnoise; keep=(signal2noise>3.5); V=(du-dv*1i)*dx/dt; %Using imaginary values to indicate direction (for convenience). Vn=abs(V); alphawarp(px,py,Vn,.2+keep*.5) quiver(px(keep),py(keep),real(V(keep))./Vn(keep),imag(V(keep))./Vn(keep),0.2,'k') %arrows show direction. caxis([0 400]) colorbar('eastoutside'); Elapsed time is 1.346849 seconds.","title":"Bindschadler Ice Stream example"},{"location":"demoengabreen/","text":"Feature tracking example This is a complete example of feature tracking on Engabreen. Load images & data Use GCPs to determine camera view direction and lens distortion parameters of image track stable rock features to determine camera shake and infer view direction of image B Pre-process DEM by filling crevasses. Track ice motion between images Georeference tracked points and calculate real world velocities. close all Setup file locations and load images & data idA = 8902; idB = 8937; % image ids (/file numbers) datafolder = 'demos'; fA = fullfile(datafolder,sprintf('IMG_%4.0f.jpg',idA)); fB = fullfile(datafolder,sprintf('IMG_%4.0f.jpg',idB)); %load images: A = imread(fA); B = imread(fB); metaA = imfinfo(fA);tA = datenum(metaA.DateTime,'yyyy:mm:dd HH:MM:SS'); metaB = imfinfo(fB);tB = datenum(metaB.DateTime,'yyyy:mm:dd HH:MM:SS'); dem = load(fullfile(datafolder,'dem')); %load DEM gcpA = load(fullfile(datafolder,'gcp8902.txt'));%load ground control points for image A Determine camera parameters for image A Initial crude guess at camera parameters Use GCPs to optimize camera parameters %calculate focal length in pixel units: FocalLength = 30; %mm (can also be found here: metaA.DigitalCamera.FocalLength) SensorSize = [22.0 14.7]; %mm: http://www.cnet.com/products/canon-eos-rebel-t3/specs/ imgsz = size(A); f = imgsz([2 1]).*(FocalLength./SensorSize); %known camera location: cameralocation = [446722.0 7396671.0 770.0]; %crude estimate of look direction. camA = camera(cameralocation,size(A),[200 0 0]*pi/180,f); %loooking west %Use GCPs to optimize the following camera parameters: %view dir, focal lengths, and a simple radial distortion model [camA,rmse,aic] = camA.optimizecam(gcpA(:,1:3),gcpA(:,4:5),'00000111110010000000'); fprintf('reprojectionerror = %3.1fpx AIC:%4.0f\\n',rmse,aic) %Visually compare the projection of the GCPs with the pixel coords: figure axes('position',[0 .1 1 .8]); hold on image(A) axis equal off ij tight hold on uv = camA.project(gcpA(:,1:3)); h = plot(gcpA(:,4),gcpA(:,5),'+',uv(:,1),uv(:,2),'rx'); legend(h,'UV of GCP','projection of GCPs','location','southoutside') title(sprintf('Projection of ground control points. RMSE = %.1fpx',rmse)) reprojectionerror = 5.1px AIC: 155 Determine view direction of camera B. find movement of rock features between images A and B determine camera B by pertubing viewdir of camera A. % First get an approximate estimate of the image shift using a single large % template [duoffset,dvoffset] = templatematch(A,B,3000,995,'templatewidth',261,'searchwidth',400,'supersample',0.5,'showprogress',false) % Get a whole bunch of image shift estimates using a grid of probe points. % Having multiple shift estimates will allow us to determine camera % rotation. [pu,pv] = meshgrid(200:700:4000,100:400:1000); pu = pu(:); pv = pv(:)+pu/10; [du,dv,C] = templatematch(A,B,pu,pv,'templatewidth',61,'searchwidth',81,'supersample',3,'initialdu',duoffset,'initialdv',dvoffset); % Determine camera rotation between A and B from the set of image % shifts. % find 3d coords consistent with the 2d pixel coords in points. xyz = camA.invproject([pu pv]); % the projection of xyz has to match the shifted coords in points+dxy: [camB,rmse] = camA.optimizecam(xyz,[pu+du pv+dv],'00000111000000000000'); %optimize 3 view direction angles to determine camera B. rmse %quantify the shift between A and B in terms of an delta angle. DeltaViewDirection = (camB.viewdir-camA.viewdir)*180/pi duoffset = 13.679 dvoffset = -1.244 rmse = 0.4059 DeltaViewDirection = 0.12633 0.017584 0.015242 Generate a set of points to be tracked between images Generate a regular grid of candidate points in world coordinates. Cull the set of candidate points to those that are visible and glaciated % The viewshed is all the points of the dem that are visible from the % camera location. They may not be in the field of view of the lens. dem.visible = voxelviewshed(dem.X,dem.Y,dem.filled,camA.xyz); %Make a regular 50 m grid of points we would like to track from image A [XA,YA] = meshgrid(min(dem.x):50:max(dem.x),min(dem.y):50:max(dem.y)); ZA = interp2(dem.X,dem.Y,dem.filled,XA,YA); %Figure out which pixel coordinates they correspond to: [uvA,~,inframe] = camA.project([XA(:) YA(:) ZA(:)]); %where would the candidate points be in image A %Insert nans where we do not want to track: keepers = double(dem.visible&dem.mask); %visible & glaciated dem points keepers = filter2(ones(11)/(11^2),keepers); %throw away points close to the edge of visibility keepers = interp2(dem.X,dem.Y,keepers,X(:),Y(:))>.99; %which candidate points fullfill the criteria. uvA(~(keepers&inframe)) = nan; Track points between images. % calculate where points would be in image B if no ice motion % ( i.e. accounting only for camera shake) camshake = camB.project(camA.invproject(uvA))-uvA; options = []; options.pu = uvA(:,1); options.pv = uvA(:,2); options.method = 'OC'; options.showprogress = true; options.searchwidth = 81; options.templatewidth = 21; options.supersample = 2; %supersample the input images for better subpixel estimation options.initialdu = camshake(:,1); options.initialdv = camshake(:,2); [du,dv,C,Cnoise] = templatematch(A,B,options); uvB = uvA+[du dv]; signal2noise = C./Cnoise; Georeference tracked points ... and calculate velocities xyzA = camA.invproject(uvA,dem.X,dem.Y,dem.filled); % has to be recalculated because uvA has been rounded. xyzB = camB.invproject(uvB,dem.X,dem.Y,dem.filled-dem.mask*22.75*(tB-tA)/365); % impose a thinning of the DEM of 23m/yr between images. V = (xyzB-xyzA)./(tB-tA); % 3d velocity. Vx = reshape(V(:,1),size(XA)); Vy = reshape(V(:,2),size(YA)); Vz = reshape(V(:,3),size(ZA)); figure; showimg(dem.x,dem.y,dem.rgb); hold on Vn = sqrt(sum(V(:,1:2).^2,2)); keep = signal2noise>2 & C>.7; alphawarp(XA,YA,sqrt(Vx.^2+Vy.^2)) quiver(xyzA(keep,1),xyzA(keep,2),V(keep,1)./Vn(keep),V(keep,2)./Vn(keep),.2,'k') caxis([0 1]) colormap hot hcb = colorbar('southoutside'); plot(camA.xyz(1),camA.xyz(2),'r+') title('Velocity in metres per day') Project velocity onto downhill slope direction ---- The largest error in the velocities will along the view direction vector. By projecting to the slope direction we strongly suppress errors arising from this. [gradX,gradY] = gradient(dem.filled,dem.X(2,2)-dem.X(1,1),dem.Y(2,2)-dem.Y(1,1)); gradN = sqrt(gradX.^2+gradY.^2); gradX = -gradX./gradN;gradY = -gradY./gradN; gradX = interp2(dem.X,dem.Y,gradX,XA,YA); gradY = interp2(dem.X,dem.Y,gradY,XA,YA); Vgn = Vx.*gradX+Vy.*gradY;%Velocity along glacier Vacross = Vx.*gradY-Vy.*gradX; %Velocity across glacier keep = reshape(keep,size(XA)); %We do not trust regions with large across glacier flow. We may get large errors where the motion is %in and out of the frame. I.e. in places where the glacier surface is %viewed very obliquely. In those places we do not have a sufficiently good %view to calculate velocities. We apply a filter to remove these. keep = keep&(abs(Vgn)>abs(Vacross)); close all figure showimg(dem.x,dem.y,dem.rgb); axis equal xy off tight hold on alphawarp(XA,YA,Vgn,keep*.7) quiver(XA(keep),YA(keep),gradX(keep),gradY(keep),.2,'k') caxis([0 1]) colormap hot hcb = colorbar('southoutside'); plot(camA.xyz(1),camA.xyz(2),'r+') title('Velocity along slope direction in metres per day')","title":"Demoengabreen"},{"location":"demoengabreen/#feature-tracking-example","text":"This is a complete example of feature tracking on Engabreen. Load images & data Use GCPs to determine camera view direction and lens distortion parameters of image track stable rock features to determine camera shake and infer view direction of image B Pre-process DEM by filling crevasses. Track ice motion between images Georeference tracked points and calculate real world velocities. close all","title":"Feature tracking example"},{"location":"demoengabreen/#setup-file-locations-and-load-images-data","text":"idA = 8902; idB = 8937; % image ids (/file numbers) datafolder = 'demos'; fA = fullfile(datafolder,sprintf('IMG_%4.0f.jpg',idA)); fB = fullfile(datafolder,sprintf('IMG_%4.0f.jpg',idB)); %load images: A = imread(fA); B = imread(fB); metaA = imfinfo(fA);tA = datenum(metaA.DateTime,'yyyy:mm:dd HH:MM:SS'); metaB = imfinfo(fB);tB = datenum(metaB.DateTime,'yyyy:mm:dd HH:MM:SS'); dem = load(fullfile(datafolder,'dem')); %load DEM gcpA = load(fullfile(datafolder,'gcp8902.txt'));%load ground control points for image A","title":"Setup file locations and load images &amp; data"},{"location":"demoengabreen/#determine-camera-parameters-for-image-a","text":"Initial crude guess at camera parameters Use GCPs to optimize camera parameters %calculate focal length in pixel units: FocalLength = 30; %mm (can also be found here: metaA.DigitalCamera.FocalLength) SensorSize = [22.0 14.7]; %mm: http://www.cnet.com/products/canon-eos-rebel-t3/specs/ imgsz = size(A); f = imgsz([2 1]).*(FocalLength./SensorSize); %known camera location: cameralocation = [446722.0 7396671.0 770.0]; %crude estimate of look direction. camA = camera(cameralocation,size(A),[200 0 0]*pi/180,f); %loooking west %Use GCPs to optimize the following camera parameters: %view dir, focal lengths, and a simple radial distortion model [camA,rmse,aic] = camA.optimizecam(gcpA(:,1:3),gcpA(:,4:5),'00000111110010000000'); fprintf('reprojectionerror = %3.1fpx AIC:%4.0f\\n',rmse,aic) %Visually compare the projection of the GCPs with the pixel coords: figure axes('position',[0 .1 1 .8]); hold on image(A) axis equal off ij tight hold on uv = camA.project(gcpA(:,1:3)); h = plot(gcpA(:,4),gcpA(:,5),'+',uv(:,1),uv(:,2),'rx'); legend(h,'UV of GCP','projection of GCPs','location','southoutside') title(sprintf('Projection of ground control points. RMSE = %.1fpx',rmse)) reprojectionerror = 5.1px AIC: 155","title":"Determine camera parameters for image A"},{"location":"demoengabreen/#determine-view-direction-of-camera-b","text":"find movement of rock features between images A and B determine camera B by pertubing viewdir of camera A. % First get an approximate estimate of the image shift using a single large % template [duoffset,dvoffset] = templatematch(A,B,3000,995,'templatewidth',261,'searchwidth',400,'supersample',0.5,'showprogress',false) % Get a whole bunch of image shift estimates using a grid of probe points. % Having multiple shift estimates will allow us to determine camera % rotation. [pu,pv] = meshgrid(200:700:4000,100:400:1000); pu = pu(:); pv = pv(:)+pu/10; [du,dv,C] = templatematch(A,B,pu,pv,'templatewidth',61,'searchwidth',81,'supersample',3,'initialdu',duoffset,'initialdv',dvoffset); % Determine camera rotation between A and B from the set of image % shifts. % find 3d coords consistent with the 2d pixel coords in points. xyz = camA.invproject([pu pv]); % the projection of xyz has to match the shifted coords in points+dxy: [camB,rmse] = camA.optimizecam(xyz,[pu+du pv+dv],'00000111000000000000'); %optimize 3 view direction angles to determine camera B. rmse %quantify the shift between A and B in terms of an delta angle. DeltaViewDirection = (camB.viewdir-camA.viewdir)*180/pi duoffset = 13.679 dvoffset = -1.244 rmse = 0.4059 DeltaViewDirection = 0.12633 0.017584 0.015242","title":"Determine view direction of camera B."},{"location":"demoengabreen/#generate-a-set-of-points-to-be-tracked-between-images","text":"Generate a regular grid of candidate points in world coordinates. Cull the set of candidate points to those that are visible and glaciated % The viewshed is all the points of the dem that are visible from the % camera location. They may not be in the field of view of the lens. dem.visible = voxelviewshed(dem.X,dem.Y,dem.filled,camA.xyz); %Make a regular 50 m grid of points we would like to track from image A [XA,YA] = meshgrid(min(dem.x):50:max(dem.x),min(dem.y):50:max(dem.y)); ZA = interp2(dem.X,dem.Y,dem.filled,XA,YA); %Figure out which pixel coordinates they correspond to: [uvA,~,inframe] = camA.project([XA(:) YA(:) ZA(:)]); %where would the candidate points be in image A %Insert nans where we do not want to track: keepers = double(dem.visible&dem.mask); %visible & glaciated dem points keepers = filter2(ones(11)/(11^2),keepers); %throw away points close to the edge of visibility keepers = interp2(dem.X,dem.Y,keepers,X(:),Y(:))>.99; %which candidate points fullfill the criteria. uvA(~(keepers&inframe)) = nan;","title":"Generate a set of points to be tracked between images"},{"location":"demoengabreen/#track-points-between-images","text":"% calculate where points would be in image B if no ice motion % ( i.e. accounting only for camera shake) camshake = camB.project(camA.invproject(uvA))-uvA; options = []; options.pu = uvA(:,1); options.pv = uvA(:,2); options.method = 'OC'; options.showprogress = true; options.searchwidth = 81; options.templatewidth = 21; options.supersample = 2; %supersample the input images for better subpixel estimation options.initialdu = camshake(:,1); options.initialdv = camshake(:,2); [du,dv,C,Cnoise] = templatematch(A,B,options); uvB = uvA+[du dv]; signal2noise = C./Cnoise;","title":"Track points between images."},{"location":"demoengabreen/#georeference-tracked-points","text":"... and calculate velocities xyzA = camA.invproject(uvA,dem.X,dem.Y,dem.filled); % has to be recalculated because uvA has been rounded. xyzB = camB.invproject(uvB,dem.X,dem.Y,dem.filled-dem.mask*22.75*(tB-tA)/365); % impose a thinning of the DEM of 23m/yr between images. V = (xyzB-xyzA)./(tB-tA); % 3d velocity. Vx = reshape(V(:,1),size(XA)); Vy = reshape(V(:,2),size(YA)); Vz = reshape(V(:,3),size(ZA)); figure; showimg(dem.x,dem.y,dem.rgb); hold on Vn = sqrt(sum(V(:,1:2).^2,2)); keep = signal2noise>2 & C>.7; alphawarp(XA,YA,sqrt(Vx.^2+Vy.^2)) quiver(xyzA(keep,1),xyzA(keep,2),V(keep,1)./Vn(keep),V(keep,2)./Vn(keep),.2,'k') caxis([0 1]) colormap hot hcb = colorbar('southoutside'); plot(camA.xyz(1),camA.xyz(2),'r+') title('Velocity in metres per day')","title":"Georeference tracked points"},{"location":"demoengabreen/#project-velocity-onto-downhill-slope-direction","text":"---- The largest error in the velocities will along the view direction vector. By projecting to the slope direction we strongly suppress errors arising from this. [gradX,gradY] = gradient(dem.filled,dem.X(2,2)-dem.X(1,1),dem.Y(2,2)-dem.Y(1,1)); gradN = sqrt(gradX.^2+gradY.^2); gradX = -gradX./gradN;gradY = -gradY./gradN; gradX = interp2(dem.X,dem.Y,gradX,XA,YA); gradY = interp2(dem.X,dem.Y,gradY,XA,YA); Vgn = Vx.*gradX+Vy.*gradY;%Velocity along glacier Vacross = Vx.*gradY-Vy.*gradX; %Velocity across glacier keep = reshape(keep,size(XA)); %We do not trust regions with large across glacier flow. We may get large errors where the motion is %in and out of the frame. I.e. in places where the glacier surface is %viewed very obliquely. In those places we do not have a sufficiently good %view to calculate velocities. We apply a filter to remove these. keep = keep&(abs(Vgn)>abs(Vacross)); close all figure showimg(dem.x,dem.y,dem.rgb); axis equal xy off tight hold on alphawarp(XA,YA,Vgn,keep*.7) quiver(XA(keep),YA(keep),gradX(keep),gradY(keep),.2,'k') caxis([0 1]) colormap hot hcb = colorbar('southoutside'); plot(camA.xyz(1),camA.xyz(2),'r+') title('Velocity along slope direction in metres per day')","title":"Project velocity onto downhill slope direction"},{"location":"demos/","text":"Demos included in the package. Engabreen demo A terrestrial time-lapse example Batura glacier A Landsat 7 tracking example Bindschadler Ice Stream D Another landsat example Schneefernerkopf A terrestrial photo rectification example.","title":"Demos"},{"location":"demos/#demos-included-in-the-package","text":"","title":"Demos included in the package."},{"location":"demos/#engabreen-demo","text":"A terrestrial time-lapse example","title":"Engabreen demo"},{"location":"demos/#batura-glacier","text":"A Landsat 7 tracking example","title":"Batura glacier"},{"location":"demos/#bindschadler-ice-stream-d","text":"Another landsat example","title":"Bindschadler Ice Stream D"},{"location":"demos/#schneefernerkopf","text":"A terrestrial photo rectification example.","title":"Schneefernerkopf"},{"location":"demoschneeferner/","text":"Schneefernerkopf example using data from PRACTISE The PRACTISE (Photo Rectification And ClassificaTIon SoftwarE) open source package has an example data set from Schneefernerkopf, southern Germany (supplementary data to H\u00f6rer et al. 2013) . Here, we show how you can use the ImGRAFT camera model on this data set. The example data set is also attached below. H\u00f6rer, S., Bernhardt, M., Corripio, J. G., & Schulz, K. (2013). PRACTISE Photo Rectification And ClassificaTIon SoftwarE (V. 1.0). Geoscientific Model Development Note this example needs the matlab mapping toolbox for loading the DEM. close all %load data datafolder=downloadDemoData('practise'); A=imread(fullfile(datafolder,'ufs20110511_0815_4dot5Mpx.jpg')); gcpA=load(fullfile(datafolder,'GCPortho6_4dot5Mpx.txt')); load(fullfile(datafolder,'dem30m.mat')); [X,Y]=meshgrid(x,y); %camera location as given in practise: camxyz=[649299.97, 5253358.26]; camxyz(3)=interp2(X,Y,dem,camxyz(1),camxyz(2),'spline')+1.5; %initial guess camera parameters: (also from practise) FocalLength=31; %mm SensorSize=[22.3 14.9]; %mm imgsz=size(A); f=imgsz([2 1]).*(FocalLength./SensorSize); camA=camera(camxyz,size(A),[225 0 0]*pi/180,f); %approximate look direction. % In this example I allow the camera-z coordinate to be free because the DEM % is only 30m resolution, and also because the viewshed appears inconsistent % with the 1.5m elevation. I also fit a 1 parameter radial distortion model % because AIC tells me that it is a better model. % % We optimize camera elevation, 3 viewdir angles, 2 focal lengths, 1 radial distortion coefficient. [camA,rmse,aic]=camA.optimizecam(gcpA(:,1:3),gcpA(:,4:5),'00100111110010000000'); fprintf('reprojectionerror=%3.1fpx AIC:%4.0f\\n',rmse,aic) %visualize the output image(A) axis equal off tight hold on %project the DEM onto the camera plane [uvDEM,~,inframe]=camA.project([X(:),Y(:),dem(:)]); %Hide DEM points that are not visible from the camera: vis=voxelviewshed(X,Y,dem,camA.xyz); uvDEM(~(inframe&vis(:)),:)=nan; %show DEM as a mesh with labelled contours on top. mesh(reshape(uvDEM(:,1),size(dem)),reshape(uvDEM(:,2),size(dem)),dem*0,'facecolor','none','edgecolor',[.7 .7 1]*.7) [c,h]=contour(reshape(uvDEM(:,1),size(dem)),reshape(uvDEM(:,2),size(dem)),dem,[2600:50:3000],'k'); clabel(c,h) %Show GCPs and reprojected GCPs uvGCP=camA.project(gcpA(:,1:3)); h=plot(uvGCP(:,1),uvGCP(:,2),'ro',gcpA(:,4),gcpA(:,5),'m*','markerfacecolor','w'); legend(h([2 1]),'UV of GCP','projection of GCPs','location','northeast') title(sprintf('Projection of ground control points. RMSE=%.1fpx',rmse)) Warning: Columns of data containing NaN values have been ignored during interpolation. reprojectionerror=1.6px AIC: 16","title":"Schneefernerkopf example using data from PRACTISE"},{"location":"demoschneeferner/#schneefernerkopf-example-using-data-from-practise","text":"The PRACTISE (Photo Rectification And ClassificaTIon SoftwarE) open source package has an example data set from Schneefernerkopf, southern Germany (supplementary data to H\u00f6rer et al. 2013) . Here, we show how you can use the ImGRAFT camera model on this data set. The example data set is also attached below. H\u00f6rer, S., Bernhardt, M., Corripio, J. G., & Schulz, K. (2013). PRACTISE Photo Rectification And ClassificaTIon SoftwarE (V. 1.0). Geoscientific Model Development Note this example needs the matlab mapping toolbox for loading the DEM. close all %load data datafolder=downloadDemoData('practise'); A=imread(fullfile(datafolder,'ufs20110511_0815_4dot5Mpx.jpg')); gcpA=load(fullfile(datafolder,'GCPortho6_4dot5Mpx.txt')); load(fullfile(datafolder,'dem30m.mat')); [X,Y]=meshgrid(x,y); %camera location as given in practise: camxyz=[649299.97, 5253358.26]; camxyz(3)=interp2(X,Y,dem,camxyz(1),camxyz(2),'spline')+1.5; %initial guess camera parameters: (also from practise) FocalLength=31; %mm SensorSize=[22.3 14.9]; %mm imgsz=size(A); f=imgsz([2 1]).*(FocalLength./SensorSize); camA=camera(camxyz,size(A),[225 0 0]*pi/180,f); %approximate look direction. % In this example I allow the camera-z coordinate to be free because the DEM % is only 30m resolution, and also because the viewshed appears inconsistent % with the 1.5m elevation. I also fit a 1 parameter radial distortion model % because AIC tells me that it is a better model. % % We optimize camera elevation, 3 viewdir angles, 2 focal lengths, 1 radial distortion coefficient. [camA,rmse,aic]=camA.optimizecam(gcpA(:,1:3),gcpA(:,4:5),'00100111110010000000'); fprintf('reprojectionerror=%3.1fpx AIC:%4.0f\\n',rmse,aic) %visualize the output image(A) axis equal off tight hold on %project the DEM onto the camera plane [uvDEM,~,inframe]=camA.project([X(:),Y(:),dem(:)]); %Hide DEM points that are not visible from the camera: vis=voxelviewshed(X,Y,dem,camA.xyz); uvDEM(~(inframe&vis(:)),:)=nan; %show DEM as a mesh with labelled contours on top. mesh(reshape(uvDEM(:,1),size(dem)),reshape(uvDEM(:,2),size(dem)),dem*0,'facecolor','none','edgecolor',[.7 .7 1]*.7) [c,h]=contour(reshape(uvDEM(:,1),size(dem)),reshape(uvDEM(:,2),size(dem)),dem,[2600:50:3000],'k'); clabel(c,h) %Show GCPs and reprojected GCPs uvGCP=camA.project(gcpA(:,1:3)); h=plot(uvGCP(:,1),uvGCP(:,2),'ro',gcpA(:,4),gcpA(:,5),'m*','markerfacecolor','w'); legend(h([2 1]),'UV of GCP','projection of GCPs','location','northeast') title(sprintf('Projection of ground control points. RMSE=%.1fpx',rmse)) Warning: Columns of data containing NaN values have been ignored during interpolation. reprojectionerror=1.6px AIC: 16","title":"Schneefernerkopf example using data from PRACTISE"},{"location":"faq/","text":"FAQ Frequently asked questions and tips How do i get velocities from Landsat images? Monte carlo sampling of uncertainties Preprocessing images for better template matching. Nice 3D plots of velocities Tips for batch processing many images","title":"FAQ"},{"location":"faq/#faq","text":"Frequently asked questions and tips How do i get velocities from Landsat images? Monte carlo sampling of uncertainties Preprocessing images for better template matching. Nice 3D plots of velocities Tips for batch processing many images","title":"FAQ"},{"location":"faq_batch_processing/","text":"Tip: Batch processing of a time lapse sequence When batch processing a time-lapse sequence then it is a good idea to separate the work flow into smaller digestible chunks and then save the results along the way. This outline shows how you might construct such a script. Determine camera orientation and internal parameters of a master image. Get as many GCPs for that image as possible. Use camera.optimizecam to determine unknown parameters. (see Schneefernerkopf for an example). Determine the camera orientation of all the remaining images with respect to master image. This can be done by tracking the features of background movement. See the Engabreen demo for an example. Pick a set of real world coordinates to feature track. See the Engabreen demo for an example. Determine the pixel displacement of all candidate image pairs. Some proposed criteria that can be used to select candidate pairs: Roughly same time of day (to minimize issues from shadow movement). E.g. dt>1day and dt<14days. Criteria that removes ill-suited images: under-exposed, cloudy, ice/water on lens. This can be done using manually or automatically using e.g. image statistics, or exposure settings. For each pair: Convert all pixel locations of tracked features to 3d positions, and then to velocities. See Engabreen for an example. I would script each of these steps separately and save the results along the way. Many of these steps require you to do some task for each file. Here's how you would typically solve that in Matlab: files=dir(fullfile(imagefolder,'img_*.jpg')); for ii=1:length(files) fname=fullfile(imagefolder,files(ii).name); [... insert your code here ...] end Refinements: There can be a slight error in calculating the change in view direction of the image with respect to the master image. Especially if the light and/or snow conditions are very different between images. This introduces a small error in the 2d->3d coordinates projection. You can reduce this error by having multiple master images depending on the conditions (early/late season, direct/diffuse light, morning/evening). When you calculate velocities then you are differencing two the locations derived from an image pair. If part of the location error is shared then this will cancel out. It can therefore be advantageous to derive the camera parameters for image B from camera A rather than from the master camera.","title":"Faq batch processing"},{"location":"faq_batch_processing/#tip-batch-processing-of-a-time-lapse-sequence","text":"When batch processing a time-lapse sequence then it is a good idea to separate the work flow into smaller digestible chunks and then save the results along the way. This outline shows how you might construct such a script. Determine camera orientation and internal parameters of a master image. Get as many GCPs for that image as possible. Use camera.optimizecam to determine unknown parameters. (see Schneefernerkopf for an example). Determine the camera orientation of all the remaining images with respect to master image. This can be done by tracking the features of background movement. See the Engabreen demo for an example. Pick a set of real world coordinates to feature track. See the Engabreen demo for an example. Determine the pixel displacement of all candidate image pairs. Some proposed criteria that can be used to select candidate pairs: Roughly same time of day (to minimize issues from shadow movement). E.g. dt>1day and dt<14days. Criteria that removes ill-suited images: under-exposed, cloudy, ice/water on lens. This can be done using manually or automatically using e.g. image statistics, or exposure settings. For each pair: Convert all pixel locations of tracked features to 3d positions, and then to velocities. See Engabreen for an example. I would script each of these steps separately and save the results along the way. Many of these steps require you to do some task for each file. Here's how you would typically solve that in Matlab: files=dir(fullfile(imagefolder,'img_*.jpg')); for ii=1:length(files) fname=fullfile(imagefolder,files(ii).name); [... insert your code here ...] end","title":"Tip: Batch processing of a time lapse sequence"},{"location":"faq_batch_processing/#refinements","text":"There can be a slight error in calculating the change in view direction of the image with respect to the master image. Especially if the light and/or snow conditions are very different between images. This introduces a small error in the 2d->3d coordinates projection. You can reduce this error by having multiple master images depending on the conditions (early/late season, direct/diffuse light, morning/evening). When you calculate velocities then you are differencing two the locations derived from an image pair. If part of the location error is shared then this will cancel out. It can therefore be advantageous to derive the camera parameters for image B from camera A rather than from the master camera.","title":"Refinements:"},{"location":"faq_landsat/","text":"How do i get velocities from Landsat images? Here are a few tips: You can download landsat scenes from Earth Explorer. Usually it is best to use the high-resolution pan-chromatic band in feature tracking. It is important that both images are taken from the same view point as the georectification in the L1G product is not 100% exact. However, if you choose an image pair where both images share the same viewpoint then these georectification errors will tend to cancel out. You should therefore only track features in images that share the same row and path number. Landsat scenes are rather large, and you may run into memory issues if you attempt to track whole scenes at once. I have written a small function that lets you load a smaller study area of a large geotiff. It is called geoimread and can be downloaded on matlabcentral. It is generally a good idea to high-pass filter the input images as a pre-processing step. See this FAQ on preprocessing for how to do that. Once you have loaded the images, and applied any pre-filtering, then you can just call templatematch to get the displacement. Here you can use these two examples as templates: Batura, Bindschadler. A 'default' template size that usually works is around 21 pixels on a side. But larger templates may work better in slow moving regions. Larger templates have the disadvantage that they are more sensitive to shear. So, large templates work best when ice flow is relatively uniform on the spatial scales of the template. In areas with slow ice flow, you need larger temporal baselines in the image pairs to accurately estimate movement. It can be a good idea to make an animated gif of the two images in the image pair. It is a great way to get an intuition about what is going on. It is great for trouble shooting.","title":"Faq landsat"},{"location":"faq_landsat/#how-do-i-get-velocities-from-landsat-images","text":"Here are a few tips: You can download landsat scenes from Earth Explorer. Usually it is best to use the high-resolution pan-chromatic band in feature tracking. It is important that both images are taken from the same view point as the georectification in the L1G product is not 100% exact. However, if you choose an image pair where both images share the same viewpoint then these georectification errors will tend to cancel out. You should therefore only track features in images that share the same row and path number. Landsat scenes are rather large, and you may run into memory issues if you attempt to track whole scenes at once. I have written a small function that lets you load a smaller study area of a large geotiff. It is called geoimread and can be downloaded on matlabcentral. It is generally a good idea to high-pass filter the input images as a pre-processing step. See this FAQ on preprocessing for how to do that. Once you have loaded the images, and applied any pre-filtering, then you can just call templatematch to get the displacement. Here you can use these two examples as templates: Batura, Bindschadler. A 'default' template size that usually works is around 21 pixels on a side. But larger templates may work better in slow moving regions. Larger templates have the disadvantage that they are more sensitive to shear. So, large templates work best when ice flow is relatively uniform on the spatial scales of the template. In areas with slow ice flow, you need larger temporal baselines in the image pairs to accurately estimate movement. It can be a good idea to make an animated gif of the two images in the image pair. It is a great way to get an intuition about what is going on. It is great for trouble shooting.","title":"How do i get velocities from Landsat images?"},{"location":"faq_montecarlo/","text":"Monte Carlo sampling of uncertainties Monte Carlo sampling of uncertainties from different sources is a simple way to propagate uncertainties through the entire processing chain. On this page we show how you can use this approach to estimate the 3d uncertainties arising from uncertainties in the DEM and in the ground control points. The method is however, much more generally applicable and it is relatively simple to expand on this to get the uncertainties in 3d velocities given uncertainties in DEM, GCPs, camera-location, subpixel feature tracking uncertainties, etc. MC Example: DEM & GCP uncertainties at Schneefernerkopf. Uncertainties in e.g. the DEM or the GCPs will propagate through the processing chain to the final estimate of the 3D position associated with a given pixel. On this page we outline how you can use a relatively simple Monte Carlo sampling of the input uncertainties to estimate the uncertainty in the final outcome. We start from the data and code in the Schneefernerkopf example. Lets say we have run the Schneefernerkopf example and now we want to know what is the position of a target pixel. First we run demoschneeferner which fits a model camera to the GCPs. To obtain the world coordinates of the target pixel then we would make an inverse projection (raytracing) of the pixel onto the DEM. demoschneeferner pixel=[875 1394]; world=camA.invproject(pixel,X,Y,dem) The question is how uncertain is the resulting world coordinate because of uncertainties in the DEM and the GCPs. Lets say that the vertical standard uncertainty is 2m in the DEM, and that the pixel uncertainties of the GCPs have a standard uncertainty of 2 pixels. In the Monte Carlo we repeat the calculation but perturb the inputs in accordance with these uncertainties and see what impact it has on the output. sigmaDEM = 2; %estimated uncertainty in DEM. (m) sigmaGCP = 2; %GCP pixel uncertainty. mcworld=nan(1000,3); %this is where we store the monte carlo sample of 3d coordinates %As we will perturb the dem in the monte carlo we might end up in a situation %where the camera appears to be below the DEM. For that reason we hide the %foreground from view by making a hole in the DEM around the camera. camdist=sqrt((X-camxyz(1)).^2+(Y-camxyz(2)).^2); dem(camdist<100)=nan; for ii=1:size(mcworld,1) mcgcpAuv=gcpA(:,4:5)+randn(size(gcpA,1),2)*sigmaGCP/sqrt(2); mccam=camA.optimizecam(gcpA(:,1:3),mcgcpAuv,'00000111110011000000'); mcdemerror=randn*sigmaDEM; mcworld(ii,:)=mccam.invproject(pixel,X,Y,dem+mcdemerror,world(1:2)); timedwaitbar(ii/length(mcworld)) end close all pcolor(X/1000,Y/1000,dem) shading interp colormap gray hold on h=plot(world(1)/1000,world(2)/1000,'rx',mcworld(:,1)/1000,mcworld(:,2)/1000,'b.',camxyz(1)/1000,camxyz(2)/1000,'cs') xlabel('Easting (km)') ylabel('Northing (km)') axis equal ylim([5252.5 5253.5]) legend('DEM','estimated pos',sprintf('MC pos \\\\sigma=[%.0fm %.0fm %.0fm]',nanstd(mcworld)), ... 'camera','location','south') We find uncertainties of 4,6, and 1m on the x,y, and z coordinates. We can also see that the x and y uncertainties covary and the uncertainty is predominantly in the distance to the camera. You can use a similar MC approach to estimate the propagated uncertainties associated with: * Uncertainties in templatematch displacements. (provided you have an estimate of how good the templatematcher works on your dataset). * Uncertainties in camera parameters such as the position. * Uncertainties in xyz position rather than the uv coordinates of the GCPs. etc. This illustrates one of the benefits of ImGRAFT being a scriptable tool: It enables these sorts of approaches.","title":"Faq montecarlo"},{"location":"faq_montecarlo/#monte-carlo-sampling-of-uncertainties","text":"Monte Carlo sampling of uncertainties from different sources is a simple way to propagate uncertainties through the entire processing chain. On this page we show how you can use this approach to estimate the 3d uncertainties arising from uncertainties in the DEM and in the ground control points. The method is however, much more generally applicable and it is relatively simple to expand on this to get the uncertainties in 3d velocities given uncertainties in DEM, GCPs, camera-location, subpixel feature tracking uncertainties, etc.","title":"Monte Carlo sampling of uncertainties"},{"location":"faq_montecarlo/#mc-example-dem-gcp-uncertainties-at-schneefernerkopf","text":"Uncertainties in e.g. the DEM or the GCPs will propagate through the processing chain to the final estimate of the 3D position associated with a given pixel. On this page we outline how you can use a relatively simple Monte Carlo sampling of the input uncertainties to estimate the uncertainty in the final outcome. We start from the data and code in the Schneefernerkopf example. Lets say we have run the Schneefernerkopf example and now we want to know what is the position of a target pixel. First we run demoschneeferner which fits a model camera to the GCPs. To obtain the world coordinates of the target pixel then we would make an inverse projection (raytracing) of the pixel onto the DEM. demoschneeferner pixel=[875 1394]; world=camA.invproject(pixel,X,Y,dem) The question is how uncertain is the resulting world coordinate because of uncertainties in the DEM and the GCPs. Lets say that the vertical standard uncertainty is 2m in the DEM, and that the pixel uncertainties of the GCPs have a standard uncertainty of 2 pixels. In the Monte Carlo we repeat the calculation but perturb the inputs in accordance with these uncertainties and see what impact it has on the output. sigmaDEM = 2; %estimated uncertainty in DEM. (m) sigmaGCP = 2; %GCP pixel uncertainty. mcworld=nan(1000,3); %this is where we store the monte carlo sample of 3d coordinates %As we will perturb the dem in the monte carlo we might end up in a situation %where the camera appears to be below the DEM. For that reason we hide the %foreground from view by making a hole in the DEM around the camera. camdist=sqrt((X-camxyz(1)).^2+(Y-camxyz(2)).^2); dem(camdist<100)=nan; for ii=1:size(mcworld,1) mcgcpAuv=gcpA(:,4:5)+randn(size(gcpA,1),2)*sigmaGCP/sqrt(2); mccam=camA.optimizecam(gcpA(:,1:3),mcgcpAuv,'00000111110011000000'); mcdemerror=randn*sigmaDEM; mcworld(ii,:)=mccam.invproject(pixel,X,Y,dem+mcdemerror,world(1:2)); timedwaitbar(ii/length(mcworld)) end close all pcolor(X/1000,Y/1000,dem) shading interp colormap gray hold on h=plot(world(1)/1000,world(2)/1000,'rx',mcworld(:,1)/1000,mcworld(:,2)/1000,'b.',camxyz(1)/1000,camxyz(2)/1000,'cs') xlabel('Easting (km)') ylabel('Northing (km)') axis equal ylim([5252.5 5253.5]) legend('DEM','estimated pos',sprintf('MC pos \\\\sigma=[%.0fm %.0fm %.0fm]',nanstd(mcworld)), ... 'camera','location','south') We find uncertainties of 4,6, and 1m on the x,y, and z coordinates. We can also see that the x and y uncertainties covary and the uncertainty is predominantly in the distance to the camera. You can use a similar MC approach to estimate the propagated uncertainties associated with: * Uncertainties in templatematch displacements. (provided you have an estimate of how good the templatematcher works on your dataset). * Uncertainties in camera parameters such as the position. * Uncertainties in xyz position rather than the uv coordinates of the GCPs. etc. This illustrates one of the benefits of ImGRAFT being a scriptable tool: It enables these sorts of approaches.","title":"MC Example: DEM &amp; GCP uncertainties at Schneefernerkopf."},{"location":"faq_nice3dplots/","text":"Nice 3D plots of velocities This example shows how you can make nice 3d plots of the output. In this case we show the downslope X,Y velocities from engabreen on top of the 3d terrain. In this code we show the velocities as 50m long cones coloured by velocity magnitude. Note: This code relies on the arrow3 function from the matlab filexchange. demoengabreen %run the engabreen example close all axes('pos',[0 0 1 1]) surface(dem.X,dem.Y,dem.Z,dem.rgb,'EdgeColor','none','FaceColor','texturemap'); view([-4 8 1]) hold on; axis equal off %h=quiver3(xyzA(keep,1),xyzA(keep,2),xyzA(keep,3)+5,Vg(keep,1)./Vgn(keep),Vg(keep,2)./Vgn(keep),Vg(keep,1)*0,'r.'); %h=quiver3(xyzA(keep,1),xyzA(keep,2),xyzA(keep,3)+5,Vg(keep,1),Vg(keep,2),Vg(keep,1)*0,'r'); from=bsxfun(@plus,xyzA(keep,:),[0 0 10]); Vdir=bsxfun(@rdivide,Vg(keep,:)*50,Vgn(keep)); % Vdir(:,3)=-20; %only show xy components of velocity cmap=jet(128); clim=[0 prctile(Vgn,90)]; caxis(clim); colors=interp1q(linspace(clim(1),clim(2),length(cmap))',cmap,max(min(Vgn(keep),clim(2)),0)); colorbar('south') for ii=1:length(from) h=arrow3(from(ii,:),Vdir(ii,:),[],1,[],'cone'); set(h,'facecolor',colors(ii,:)); end","title":"Faq nice3dplots"},{"location":"faq_nice3dplots/#nice-3d-plots-of-velocities","text":"This example shows how you can make nice 3d plots of the output. In this case we show the downslope X,Y velocities from engabreen on top of the 3d terrain. In this code we show the velocities as 50m long cones coloured by velocity magnitude. Note: This code relies on the arrow3 function from the matlab filexchange. demoengabreen %run the engabreen example close all axes('pos',[0 0 1 1]) surface(dem.X,dem.Y,dem.Z,dem.rgb,'EdgeColor','none','FaceColor','texturemap'); view([-4 8 1]) hold on; axis equal off %h=quiver3(xyzA(keep,1),xyzA(keep,2),xyzA(keep,3)+5,Vg(keep,1)./Vgn(keep),Vg(keep,2)./Vgn(keep),Vg(keep,1)*0,'r.'); %h=quiver3(xyzA(keep,1),xyzA(keep,2),xyzA(keep,3)+5,Vg(keep,1),Vg(keep,2),Vg(keep,1)*0,'r'); from=bsxfun(@plus,xyzA(keep,:),[0 0 10]); Vdir=bsxfun(@rdivide,Vg(keep,:)*50,Vgn(keep)); % Vdir(:,3)=-20; %only show xy components of velocity cmap=jet(128); clim=[0 prctile(Vgn,90)]; caxis(clim); colors=interp1q(linspace(clim(1),clim(2),length(cmap))',cmap,max(min(Vgn(keep),clim(2)),0)); colorbar('south') for ii=1:length(from) h=arrow3(from(ii,:),Vdir(ii,:),[],1,[],'cone'); set(h,'facecolor',colors(ii,:)); end","title":"Nice 3D plots of velocities"},{"location":"faq_occ/","text":"How do i do orientation correlation. Orientation correlation has been added to the templatematch function. It often out performs NCC, and has good performance on Landsat 7 images with SLC off. You can also do it manually, which used to be the solution for doing it. ImGRAFT can also do orientation correlation if you apply a simple \"orientation\" pre-processing step of the images. Here's how you might implement that: forient=@(A)exp(i*atan2(imfilter(A,[1 0 -1],'replicate'),imfilter(A,[1;0;-1],'replicate'))); oA=forient(A); oB=forient(B); [du,dv]=templatematch(oA,oB,'method','CCF') Reference: Fitch et al. 2002 CCF-O","title":"Faq occ"},{"location":"faq_occ/#how-do-i-do-orientation-correlation","text":"Orientation correlation has been added to the templatematch function. It often out performs NCC, and has good performance on Landsat 7 images with SLC off. You can also do it manually, which used to be the solution for doing it. ImGRAFT can also do orientation correlation if you apply a simple \"orientation\" pre-processing step of the images. Here's how you might implement that: forient=@(A)exp(i*atan2(imfilter(A,[1 0 -1],'replicate'),imfilter(A,[1;0;-1],'replicate'))); oA=forient(A); oB=forient(B); [du,dv]=templatematch(oA,oB,'method','CCF') Reference: Fitch et al. 2002 CCF-O","title":"How do i do orientation correlation."},{"location":"faq_preprocessing/","text":"Preprocessing images for feature tracking In some cases you may improve the feature tracking by pre-processing the images. The aim of the preprocessing may be to reduce effects from different lighting conditions, remove noise, or reduce the weight of outliers. Here are a set of proposed pre-processing steps that may improve your results. Many of the filters below are written using matlabs @-syntax for anonymous functions. You would apply the filters like this: A=im2single(imread('test.tif')); hpass=@(A,sigma)A-imfilter(A,fspecial('gaussian',sigma*3,sigma)); %define a filter function A=hpass(A,3); %apply the filter imagesc(A) High-pass filtering Feature tracking using NCC will attempt to align bright and dark regions of the image. These may correspond to illuminated vs shadow regions of the ice and thus depend strongly on the direction of the illumination. For that reason you may want to apply a high pass filter to focus on finer details than large patches of bright and dark. This can be written in matlab like this: hpass=@(A,sigma)A-imfilter(A,fspecial('gaussian',sigma*3,sigma)) Local histogram equalization Histogram equalization will bring in outliers so that they are not allowed to dominate the output. histequalfilt=@(A,tilesize)adapthisteq(A,'NumTiles',ceil(size(A)./tilesize)); See help on adapthisteq from the image processing toolbox. Noise reduction See e.g. the wiener2 function in the image processing toolbox. Combination of filters You can combine multiple filters. E.g. myfilter=@(A)histequalfilt(hpass(A,3),50); Structure texture separation Structure texture separation separates the image into a structure component which can be thought of as the background color, and a residual called the texture. In this respect the structure component is similar to a low-pass filtered image except that structure-texture methods allow for sharp transitions between regions of different brightness. I have not tried structure-texture separation techniques yet but have read papers that indicate that this may be a very fruitful venue for tracking between scenes where the sun position has changed. It could help suppress the sharp brightness change at the edge of a shadow from a mountain. I recommend this matlab package: http://www.cse.cuhk.edu.hk/leojia/projects/texturesep/index.html Gradient orientation transformation See also Orientation correlation -- This is now a built-in method of templatematch.","title":"Faq preprocessing"},{"location":"faq_preprocessing/#preprocessing-images-for-feature-tracking","text":"In some cases you may improve the feature tracking by pre-processing the images. The aim of the preprocessing may be to reduce effects from different lighting conditions, remove noise, or reduce the weight of outliers. Here are a set of proposed pre-processing steps that may improve your results. Many of the filters below are written using matlabs @-syntax for anonymous functions. You would apply the filters like this: A=im2single(imread('test.tif')); hpass=@(A,sigma)A-imfilter(A,fspecial('gaussian',sigma*3,sigma)); %define a filter function A=hpass(A,3); %apply the filter imagesc(A)","title":"Preprocessing images for feature tracking"},{"location":"faq_preprocessing/#high-pass-filtering","text":"Feature tracking using NCC will attempt to align bright and dark regions of the image. These may correspond to illuminated vs shadow regions of the ice and thus depend strongly on the direction of the illumination. For that reason you may want to apply a high pass filter to focus on finer details than large patches of bright and dark. This can be written in matlab like this: hpass=@(A,sigma)A-imfilter(A,fspecial('gaussian',sigma*3,sigma))","title":"High-pass filtering"},{"location":"faq_preprocessing/#local-histogram-equalization","text":"Histogram equalization will bring in outliers so that they are not allowed to dominate the output. histequalfilt=@(A,tilesize)adapthisteq(A,'NumTiles',ceil(size(A)./tilesize)); See help on adapthisteq from the image processing toolbox.","title":"Local histogram equalization"},{"location":"faq_preprocessing/#noise-reduction","text":"See e.g. the wiener2 function in the image processing toolbox. Combination of filters You can combine multiple filters. E.g. myfilter=@(A)histequalfilt(hpass(A,3),50);","title":"Noise reduction"},{"location":"faq_preprocessing/#structure-texture-separation","text":"Structure texture separation separates the image into a structure component which can be thought of as the background color, and a residual called the texture. In this respect the structure component is similar to a low-pass filtered image except that structure-texture methods allow for sharp transitions between regions of different brightness. I have not tried structure-texture separation techniques yet but have read papers that indicate that this may be a very fruitful venue for tracking between scenes where the sun position has changed. It could help suppress the sharp brightness change at the edge of a shadow from a mountain. I recommend this matlab package: http://www.cse.cuhk.edu.hk/leojia/projects/texturesep/index.html","title":"Structure texture separation"},{"location":"faq_preprocessing/#gradient-orientation-transformation","text":"See also Orientation correlation -- This is now a built-in method of templatematch.","title":"Gradient orientation transformation"},{"location":"functions/","text":"Key Functions and classes in the package templatematch ... is the function which tracks the displacement between two images. camera.m ... is a class which can be used to project between pixel and world coordinates. voxelviewshed.m ... is a fast function for calculating the viewshed of a dem from a given viewpoint. Utility functions showimg ... displays an image at specific coordinates. alphawarp ... drapes a semi-transparent and possibly distorted image over a figure.","title":"Functions"},{"location":"functions/#key-functions-and-classes-in-the-package","text":"","title":"Key Functions and classes in the package"},{"location":"functions/#templatematch","text":"... is the function which tracks the displacement between two images.","title":"templatematch"},{"location":"functions/#cameram","text":"... is a class which can be used to project between pixel and world coordinates.","title":"camera.m"},{"location":"functions/#voxelviewshedm","text":"... is a fast function for calculating the viewshed of a dem from a given viewpoint.","title":"voxelviewshed.m"},{"location":"functions/#utility-functions","text":"","title":"Utility functions"},{"location":"functions/#showimg","text":"... displays an image at specific coordinates.","title":"showimg"},{"location":"functions/#alphawarp","text":"... drapes a semi-transparent and possibly distorted image over a figure.","title":"alphawarp"},{"location":"installation/","text":"Installation instructions The latest version of the package can be downloaded from github . Download the zip archive with the imgraft code. Unzip the archive into a folder of your choice. Add the folder to the matlab path. You can also add it temporarily using matlabs addpath command. Test it by running the demoengabreen example.","title":"Install"},{"location":"installation/#installation-instructions","text":"The latest version of the package can be downloaded from github . Download the zip archive with the imgraft code. Unzip the archive into a folder of your choice. Add the folder to the matlab path. You can also add it temporarily using matlabs addpath command. Test it by running the demoengabreen example.","title":"Installation instructions"},{"location":"showimg/","text":"showimg.m Display an image Usage h=showimg(x,y,A) Very similar to imshow, but makes it easier to pass pixel coordinates. Example usage: A = imread('cameraman.tif'); A(1,1,3)=0; %set the green and blue channels to all zeros. x = (1:size(A,2))*10+100; y = (size(A,1):-1:1)*10+100; showimg(x,y,A); axis on","title":"Showimg"},{"location":"showimg/#showimgm","text":"Display an image Usage h=showimg(x,y,A) Very similar to imshow, but makes it easier to pass pixel coordinates. Example usage: A = imread('cameraman.tif'); A(1,1,3)=0; %set the green and blue channels to all zeros. x = (1:size(A,2))*10+100; y = (size(A,1):-1:1)*10+100; showimg(x,y,A); axis on","title":"showimg.m"},{"location":"templatematch/","text":"Feature tracking by template matching You provide templatematch with two images (A and B) and a set of coordinates in A that should be tracked. Templatematch will then cut out small templates around each point and calculate the similarity within a search region in image B. The measure of similarity will typically be normalized cross correlation. The 'optimal' displacement xy will be returned with sub-pixel precision. A simple example of template matching can be found here. In its simplest form you can use templatematch thus: [du, dv, peakCorr, meanAbsCorr, pu, pv] = templatematch(A,B) Note: u,v refers to pixel coordinates in the code below. USAGE: [du, dv, peakCorr, meanAbsCorr, pu, pv] = templatematch(A,B[,pu,pv][,parameter-value-pairs]) INPUTS A,B: images pu,pv: pixel coordinates in A that should be located in B. (Default is a regular grid) NAMED PARAMETERS: TemplateWidth,TemplateHeight: Size of templates in A (Default: 21). SearchWidth,SearchHeight: Size of search region in B (Default: TemplateWidth+40). SuperSample: super sampling factor of input images for improved subpixel accuracy. (default=1) Initialdu,Initialdv: initial guess of the displacement between A & B super: supersampling factor (input to imresize) ShowProgress: Boolean or cell-array of strings. true (default) is used for a text progress bar. A cell of strings is used to name the A & B images in a progress figure. Method: 'NCC'(default), 'NORMXCORR2' or 'PC' (normalized cross correlation or phase correlation) OUTPUTS: du,dv: displacement of each point in pu,pv. [A(pu,pv) has moved to B(pu+du,pv+dv)] peakCorr: correlation coefficient of the matched template. meanAbsCorr: The mean absolute correlation coefficitent over the search region is an estimate of the noise level. pu,pv: actual pixel centers of templates in A (may differ slightly from inputs because of rounding). Coordinates of features to be tracked (pu,pv): pu and pv are used to specify the pixel coordinates of the features in A that should be tracked. If none is specified then templatematch will drape a regular grid over the entire image. Template Width / Height: The template size is controlled with the TemplateWidth/TemplateHeight input parameters. You can specify different template sizes for each row in points. There is no choice that works well in all situations and there are trade-offs to consider. Pros of a large template size are: * Fewer false matches as the 'fingerprint' of the feature is more unique. * Cons of a large template size; * Reduced resolution of output. Hi resolution is necessary for narrow outlet glaciers and more data also helps in post-processing to find erroneous results. * Performance decreases with template size. This may be partly compensated by reducing the number of tracked points as the effective resolution is any decreased. * Possibly greater total shear inside the template. Shear reduces the cross correlation and thus makes it harder to recognize the feature in image B. So I recommend considering what is the desired output resolution and then also simply looking at the image to see what template size you would expect should work. Finally i would experiment with different template sizes to find what works best in the particular situation. Search region size (SearchWidth/SearchHeight): The search region size is controlled with the SearchWidth & SearchHeight input parameters. Generally you want to make that as small as possible to minimize the probability of false matches. I recommend using SearchWidth=TemplateWidth+max_expected_displacement . Here, you want to be generous with the max_expected_displacement, so as to not exclude the possibility you may be wrong. Sometimes there may be an advantage to choosing an even larger search region as it allows for more accurate calculation of the signal to noise ratio: One of the outputs is the average absolute correlation inside the search region. This is normally used as the noise level in a calculation of the signal to noise ratio. Super-sampling: You can specify a super sampling factor which will resample the template and search images in order to improve the sub-pixel precision. This super sampling factor can also be used to downsample the images (when super<1) to coarser resolution if you want to speed up the calculation at the expense of precision. This approach is used in the 'rock matching' of the Engabreen example. The good performance of super-sampling to achieve sub-pixel precision can be seen in Gilo and K\u00e4\u00e4b (2011). Initial guess of displacement (initialdu/initialdv): The larger the search region, the greater the chances are that other features will be visually similar to the template image being sought. Therefore it is generally a good idea to reduce the search region to being as small as possible. However, at the same time the search region has to be sufficiently wide to allow for the true displacement. By providing an initial guess for the displacement then you can reduce the search window size. Advanced TIP: you can use progressively finer scales with initial displacements taken from the preceding coarse scale matches. This approach will be similar to pyramidal approaches to feature tracking. Initialdu/initialdv can also be used as a mask by inserting nans at locations where Showprogress The showprogress input parameter can be used to open a window where the progress of the feature tracking can be followed as it is being done. The showprogress parameter can switch between three modes: If showprogress is empty or false. (default) Do not show a progress window. This is if showprogress is empty or false. If showprogress is a cell-array of image names/ids: Show the two images and the tracked points as they are being matched (colored according to the quality of the match). if showprogress is true. Show a text progressbar on the command prompt. Methods You can choose normalized cross correlation, phase correlation, and orientation correlation as a similarity measures. These measures do not allow for rotation of the template. It should, however, be relatively easy to expand with additional similarity measures. Orientation correlation (\"OC\") is the recommended choice in most cases. Outputs Displacement (du,dv) For each point in points a \"correlation\" maximum within the search region is found and the corresponding displacement is returned in du,dv. This will have the same dimensions as the du, dv inputs. Match quality (peakCorr, meanAbsCorr) peakCorr is the maximum correlation coefficient found at the location of each match. meanAbsCorr is the average or typical correlation coefficient over the entire search window. The ratio peakCorr./meanAbsCorr can be interpreted as a signal-to-noise ratio and can be used to filter false matches. Advanced tip: Sometimes it works better to spatially smooth meanAbsCorr before evaluating the signal-to-noise ratio . Pre-processing images In some cases you may improve the feature tracking by pre-processing the images. This FAQ page has a few suggestions for how you might pre-process the images.","title":"Templatematch"},{"location":"templatematch/#feature-tracking-by-template-matching","text":"You provide templatematch with two images (A and B) and a set of coordinates in A that should be tracked. Templatematch will then cut out small templates around each point and calculate the similarity within a search region in image B. The measure of similarity will typically be normalized cross correlation. The 'optimal' displacement xy will be returned with sub-pixel precision. A simple example of template matching can be found here. In its simplest form you can use templatematch thus: [du, dv, peakCorr, meanAbsCorr, pu, pv] = templatematch(A,B) Note: u,v refers to pixel coordinates in the code below. USAGE: [du, dv, peakCorr, meanAbsCorr, pu, pv] = templatematch(A,B[,pu,pv][,parameter-value-pairs]) INPUTS A,B: images pu,pv: pixel coordinates in A that should be located in B. (Default is a regular grid) NAMED PARAMETERS: TemplateWidth,TemplateHeight: Size of templates in A (Default: 21). SearchWidth,SearchHeight: Size of search region in B (Default: TemplateWidth+40). SuperSample: super sampling factor of input images for improved subpixel accuracy. (default=1) Initialdu,Initialdv: initial guess of the displacement between A & B super: supersampling factor (input to imresize) ShowProgress: Boolean or cell-array of strings. true (default) is used for a text progress bar. A cell of strings is used to name the A & B images in a progress figure. Method: 'NCC'(default), 'NORMXCORR2' or 'PC' (normalized cross correlation or phase correlation) OUTPUTS: du,dv: displacement of each point in pu,pv. [A(pu,pv) has moved to B(pu+du,pv+dv)] peakCorr: correlation coefficient of the matched template. meanAbsCorr: The mean absolute correlation coefficitent over the search region is an estimate of the noise level. pu,pv: actual pixel centers of templates in A (may differ slightly from inputs because of rounding).","title":"Feature tracking by template matching"},{"location":"templatematch/#coordinates-of-features-to-be-tracked-pupv","text":"pu and pv are used to specify the pixel coordinates of the features in A that should be tracked. If none is specified then templatematch will drape a regular grid over the entire image.","title":"Coordinates of features to be tracked (pu,pv):"},{"location":"templatematch/#template-width-height","text":"The template size is controlled with the TemplateWidth/TemplateHeight input parameters. You can specify different template sizes for each row in points. There is no choice that works well in all situations and there are trade-offs to consider. Pros of a large template size are: * Fewer false matches as the 'fingerprint' of the feature is more unique. * Cons of a large template size; * Reduced resolution of output. Hi resolution is necessary for narrow outlet glaciers and more data also helps in post-processing to find erroneous results. * Performance decreases with template size. This may be partly compensated by reducing the number of tracked points as the effective resolution is any decreased. * Possibly greater total shear inside the template. Shear reduces the cross correlation and thus makes it harder to recognize the feature in image B. So I recommend considering what is the desired output resolution and then also simply looking at the image to see what template size you would expect should work. Finally i would experiment with different template sizes to find what works best in the particular situation.","title":"Template Width / Height:"},{"location":"templatematch/#search-region-size-searchwidthsearchheight","text":"The search region size is controlled with the SearchWidth & SearchHeight input parameters. Generally you want to make that as small as possible to minimize the probability of false matches. I recommend using SearchWidth=TemplateWidth+max_expected_displacement . Here, you want to be generous with the max_expected_displacement, so as to not exclude the possibility you may be wrong. Sometimes there may be an advantage to choosing an even larger search region as it allows for more accurate calculation of the signal to noise ratio: One of the outputs is the average absolute correlation inside the search region. This is normally used as the noise level in a calculation of the signal to noise ratio.","title":"Search region size (SearchWidth/SearchHeight):"},{"location":"templatematch/#super-sampling","text":"You can specify a super sampling factor which will resample the template and search images in order to improve the sub-pixel precision. This super sampling factor can also be used to downsample the images (when super<1) to coarser resolution if you want to speed up the calculation at the expense of precision. This approach is used in the 'rock matching' of the Engabreen example. The good performance of super-sampling to achieve sub-pixel precision can be seen in Gilo and K\u00e4\u00e4b (2011).","title":"Super-sampling:"},{"location":"templatematch/#initial-guess-of-displacement-initialduinitialdv","text":"The larger the search region, the greater the chances are that other features will be visually similar to the template image being sought. Therefore it is generally a good idea to reduce the search region to being as small as possible. However, at the same time the search region has to be sufficiently wide to allow for the true displacement. By providing an initial guess for the displacement then you can reduce the search window size. Advanced TIP: you can use progressively finer scales with initial displacements taken from the preceding coarse scale matches. This approach will be similar to pyramidal approaches to feature tracking. Initialdu/initialdv can also be used as a mask by inserting nans at locations where","title":"Initial guess of displacement (initialdu/initialdv):"},{"location":"templatematch/#showprogress","text":"The showprogress input parameter can be used to open a window where the progress of the feature tracking can be followed as it is being done. The showprogress parameter can switch between three modes: If showprogress is empty or false. (default) Do not show a progress window. This is if showprogress is empty or false. If showprogress is a cell-array of image names/ids: Show the two images and the tracked points as they are being matched (colored according to the quality of the match). if showprogress is true. Show a text progressbar on the command prompt.","title":"Showprogress"},{"location":"templatematch/#methods","text":"You can choose normalized cross correlation, phase correlation, and orientation correlation as a similarity measures. These measures do not allow for rotation of the template. It should, however, be relatively easy to expand with additional similarity measures. Orientation correlation (\"OC\") is the recommended choice in most cases.","title":"Methods"},{"location":"templatematch/#outputs","text":"","title":"Outputs"},{"location":"templatematch/#displacement-dudv","text":"For each point in points a \"correlation\" maximum within the search region is found and the corresponding displacement is returned in du,dv. This will have the same dimensions as the du, dv inputs.","title":"Displacement (du,dv)"},{"location":"templatematch/#match-quality-peakcorr-meanabscorr","text":"peakCorr is the maximum correlation coefficient found at the location of each match. meanAbsCorr is the average or typical correlation coefficient over the entire search window. The ratio peakCorr./meanAbsCorr can be interpreted as a signal-to-noise ratio and can be used to filter false matches. Advanced tip: Sometimes it works better to spatially smooth meanAbsCorr before evaluating the signal-to-noise ratio .","title":"Match quality (peakCorr, meanAbsCorr)"},{"location":"templatematch/#pre-processing-images","text":"In some cases you may improve the feature tracking by pre-processing the images. This FAQ page has a few suggestions for how you might pre-process the images.","title":"Pre-processing images"},{"location":"voxelviewshed/","text":"voxelviewshed.m voxelviewshed calculates the viewshed over a DEM. The viewshed is all the locations that are potentially visible from a given location. USAGE: vis=voxelviewshed(X,Y,Z,camxyz) INPUTS: X,Y,Z: input DEM (regular grid). camxyz: 3-element vector specifying viewpoint. OUTPUT: vis: boolean visibility matrix (same size as Z) EXAMPLE: [X,Y]=meshgrid(-299:300); Z=abs(ifft2(ifftshift((hypot(Y,X)+1e-5).^(-2.1).*exp(rand(size(X))*2i*pi)))); %example terrain inspired by rafael.pinto Z=(Z-Z(300,300))/std(Z(:)); camxyz=[0,0,0.5]; vis=voxelviewshed(X,Y,Z,camxyz); clf; surf(X,Y,Z,Z.*vis-~vis*2,'EdgeColor','none','FaceColor','interp'); hold on; plot3(camxyz(1),camxyz(2),camxyz(3),'k*') camlight See also Engabreen feature tracking example for an example use case. The algorithm is custom made and very fast. Conceptually it works by draping curtains hanging down from the DEM grid points in progressively greater circles around the camera while keeping track of visually highest 'curtain point' for every compass direction.","title":"voxelviewshed.m"},{"location":"voxelviewshed/#voxelviewshedm","text":"voxelviewshed calculates the viewshed over a DEM. The viewshed is all the locations that are potentially visible from a given location. USAGE: vis=voxelviewshed(X,Y,Z,camxyz) INPUTS: X,Y,Z: input DEM (regular grid). camxyz: 3-element vector specifying viewpoint. OUTPUT: vis: boolean visibility matrix (same size as Z)","title":"voxelviewshed.m"},{"location":"voxelviewshed/#example","text":"[X,Y]=meshgrid(-299:300); Z=abs(ifft2(ifftshift((hypot(Y,X)+1e-5).^(-2.1).*exp(rand(size(X))*2i*pi)))); %example terrain inspired by rafael.pinto Z=(Z-Z(300,300))/std(Z(:)); camxyz=[0,0,0.5]; vis=voxelviewshed(X,Y,Z,camxyz); clf; surf(X,Y,Z,Z.*vis-~vis*2,'EdgeColor','none','FaceColor','interp'); hold on; plot3(camxyz(1),camxyz(2),camxyz(3),'k*') camlight See also Engabreen feature tracking example for an example use case. The algorithm is custom made and very fast. Conceptually it works by draping curtains hanging down from the DEM grid points in progressively greater circles around the camera while keeping track of visually highest 'curtain point' for every compass direction.","title":"EXAMPLE:"}]}